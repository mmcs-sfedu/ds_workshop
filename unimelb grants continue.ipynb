{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка данных и логистическая регрессия для задачи бинарной классификации (продолжение)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сегодня мы продолжим знакомиться с основными техниками предобработки данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала загрузим и инициализируем все необходимые [данные](https://yadi.sk/d/lvNm3uNm3LGvG3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('saved_data.pickle', 'rb') as f:\n",
    "    X_train_real_scaled = pickle.load(f)\n",
    "    X_test_real_scaled = pickle.load(f)\n",
    "    X_train_cat_oh = pickle.load(f)\n",
    "    X_test_cat_oh = pickle.load(f)\n",
    "    y_train = pickle.load(f)\n",
    "    y_test = pickle.load(f)\n",
    "    \n",
    "param_grid = {'C': [0.01, 0.05, 0.1, 0.5, 1, 5, 10]}\n",
    "cv=3\n",
    "\n",
    "def plot_scores(optimizer):\n",
    "    scores = [[item[0]['C'], \n",
    "               item[1], \n",
    "               (np.sum((item[2]-item[1])**2)/(item[2].size-1))**0.5] for item in optimizer.grid_scores_]\n",
    "    scores = np.array(scores)\n",
    "    plt.semilogx(scores[:,0], scores[:,1])\n",
    "    plt.fill_between(scores[:,0], scores[:,1]-scores[:,2], \n",
    "                                  scores[:,1]+scores[:,2], alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**. Выведите общее количество анализируемых объектов в обучающей выборке и количество признаков в датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# место для вашего кода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Визуализация данных в пространстве признаков\n",
    "\n",
    "Будем рассматривать наши данные как точки в N-мерном пространстве. В таком случае, есть необходимость посмотреть на это множество точек. Тогда, возможно, мы сможем увидеть где нужно провести ту самую разделяющую гиперплоскость. Проблема в том, что пространство слишком большой размерности. Разумным решением является проецирование всего пространства признаков на двумерное (трехмерное) подпространство. Одним из самых популярных метод такого проецирования является t-SNE (t-distributed Stochastic Neighbor Embedding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "projector = TSNE(n_components=2, random_state=0)\n",
    "train_2d = projector.fit_transform(X_train_real_scaled) \n",
    "plt.scatter(train_2d[y_train == 1, 0], train_2d[y_train == 1, 1], color='red', alpha = 0.5)\n",
    "plt.scatter(train_2d[y_train == 0, 0], train_2d[y_train == 0, 1], color='blue', alpha = 0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Трансформация признаков.\n",
    "\n",
    "Теперь рассмотрим способы преобразования признаков. Существует достаточно много различных способов трансформации признаков, которые позволяют при помощи линейных методов получать более сложные разделяющие поверхности. Самым базовым является полиномиальное преобразование признаков. Его идея заключается в том, что помимо самих признаков вы дополнительно включаете набор все полиномы степени $p$, которые можно из них построить. Для случая $p=2$ преобразование выглядит следующим образом:\n",
    "\n",
    "$$ \\phi(x_i) = [x_{i,1}^2, ..., x_{i,D}^2, x_{i,1}x_{i,2}, ..., x_{i,D-1}x_{i,D}, x_{i,1}, ..., x_{i,D}, 1] $$\n",
    "\n",
    "Рассмотрим принцип работы данных признаков на данных, сэмплированных из гауссиан:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "# Сэмплируем данные из первой гауссианы\n",
    "data_0 = np.random.multivariate_normal([0,0], [[0.5,0],[0,0.5]], size=40)\n",
    "# И из второй\n",
    "data_1 = np.random.multivariate_normal([0,1], [[0.5,0],[0,0.5]], size=40)\n",
    "# На обучение берём 20 объектов из первого класса и 10 из второго\n",
    "example_data_train = np.vstack([data_0[:20,:], data_1[:10,:]])\n",
    "example_labels_train = np.concatenate([np.zeros((20)), np.ones((10))])\n",
    "# На тест - 20 из первого и 30 из второго\n",
    "example_data_test = np.vstack([data_0[20:,:], data_1[10:,:]])\n",
    "example_labels_test = np.concatenate([np.zeros((20)), np.ones((30))])\n",
    "# Задаём координатную сетку, на которой будем вычислять область классификации\n",
    "xx, yy = np.meshgrid(np.arange(-3, 3, 0.02), np.arange(-3, 3, 0.02))\n",
    "\n",
    "# Инициализируем класс, который выполняет преобразование\n",
    "transform = PolynomialFeatures(2)\n",
    "# Обучаем преобразование на обучающей выборке, применяем его к тестовой\n",
    "example_data_train_poly = transform.fit_transform(example_data_train)\n",
    "example_data_test_poly = transform.transform(example_data_test)\n",
    "# Обращаем внимание на параметр fit_intercept=False\n",
    "optimizer = GridSearchCV(LR(class_weight='balanced', fit_intercept=False), param_grid, cv=cv, n_jobs=-1)\n",
    "optimizer.fit(example_data_train_poly, example_labels_train)\n",
    "Z = optimizer.predict(transform.transform(np.c_[xx.ravel(), yy.ravel()])).reshape(xx.shape)\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Pastel2)\n",
    "plt.scatter(data_0[:,0], data_0[:,1], color='red')\n",
    "plt.scatter(data_1[:,0], data_1[:,1], color='blue')\n",
    "plt.title('Balanced classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Какие выводы можно сделать, глядя на получившийся график?\n",
    "\n",
    "**Задание.** Выведите число признаков в новой модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# место для вашего кода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но при этом одновременно данный метод способствует более сильной способности модели к переобучению из-за быстрого роста числа признаком с увеличением степени $p$. Рассмотрим пример с $p=11$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = PolynomialFeatures(11)\n",
    "example_data_train_poly = transform.fit_transform(example_data_train)\n",
    "example_data_test_poly = transform.transform(example_data_test)\n",
    "optimizer = GridSearchCV(LR(class_weight='balanced', fit_intercept=False), param_grid, cv=cv, n_jobs=-1)\n",
    "optimizer.fit(example_data_train_poly, example_labels_train)\n",
    "Z = optimizer.predict(transform.transform(np.c_[xx.ravel(), yy.ravel()])).reshape(xx.shape)\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Pastel2)\n",
    "plt.scatter(data_0[:,0], data_0[:,1], color='red')\n",
    "plt.scatter(data_1[:,0], data_1[:,1], color='blue')\n",
    "plt.title('Corrected class weights')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**. Выведите количество признаков в данной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# место для вашего кода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратили ли вы внимание на то, что модель переобучилась? Посмотрите раздел \"Понимание регуляризации\" в следующей [статье](https://msdn.microsoft.com/ru-ru/magazine/dn904675.aspx).\n",
    "\n",
    "**Задание**. Можете ли вы своими словами сформулировать что такое переобучение?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 6. Трансформация вещественных признаков.\n",
    "\n",
    "1. Реализуйте по аналогии с примером преобразование вещественных признаков модели при помощи полиномиальных признаков степени 2 и 3.\n",
    "2. Постройте логистическую регрессию на новых данных, одновременно подобрав оптимальные гиперпараметры. Обращаем внимание, что в преобразованных признаках уже присутствует столбец, все значения которого равны 1, поэтому обучать дополнительно значение $b$ не нужно, его функцию выполняет один из весов $w$. В связи с этим во избежание линейной зависимости в датасете, в вызов класса логистической регрессии требуется передавать параметр fit_intercept=False. \n",
    "3. Получите AUC ROC на тесте и сравните данный результат с использованием обычных признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# место для вашего кода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# место для вашего кода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# место для вашего кода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**. Какое возведение в степень показывает себя лучше (1,2 или 3)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регрессия Lasso.\n",
    "К логистической регрессии также можно применить L1-регуляризацию (Lasso), вместо регуляризации L2, которая будет приводить к отбору признаков. Вам предлагается применить L1-регуляцию к исходным признакам и проинтерпретировать полученные результаты (применение отбора признаков к полиномиальным так же можно успешно применять, но в нём уже будет отсутствовать компонента интерпретации, т.к. смысловое значение оригинальных признаков известно, а полиномиальных - уже может быть достаточно нетривиально). Для вызова логистической регрессии с L1-регуляризацией достаточно передать параметр penalty='l1' в инициализацию класса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 7. Отбор признаков при помощи регрессии Lasso.\n",
    "\n",
    "1. Обучите регрессию Lasso на стратифицированных отмасштабированных выборках, используя балансировку классов при помощи весов.\n",
    "2. Получите ROC AUC регрессии, сравните его с предыдущими результатами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.hstack([X_train_real_scaled, X_train_cat_oh])\n",
    "X_test = np.hstack([X_test_real_scaled, X_test_cat_oh])\n",
    "param_grid['penalty'] = ['l1']\n",
    "model = analyze_cv(X_train, y_train, X_test, y_test, param_grid, cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Задание**. Выведите количество вещественных признаков, которые имеют нулевые веса в итоговой модели. Выведите количество признаков порожденных от категориальных,  которые имеют нулевые веса в итоговой модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# место для вашего кода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уже сейчас видно, что наши признаки содержат много лишнего. Будем убирать малозначимые признаки с помощью преобразователя SelectFromModel. После этого сравним результаты обучения на исходном наборе и отфильтрованном."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectFromModel(model, prefit=True)\n",
    "indices = selector.get_support()\n",
    "X_train = selector.transform(X_train)\n",
    "X_test = selector.transform(X_test)\n",
    "param_grid['penalty'] = ['l2']  # тут нужно вернуть этот параметр в исходное состояние\n",
    "model = analyze_cv(X_train, y_train, X_test, y_test, param_grid, cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**. Выведите сколько всего признаков осталось."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# место для вашего кода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**. Найдите в документации sklearn информацию о том как отбираются признаки с помощью SelectFromModel. Какое значение параметра threshold мы использовали?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**.\n",
    "1. Из множества вещественных признаков отфильтровать малозначимые.\n",
    "2. Визуализировать это отфильтрованное множество вещественных признаков.\n",
    "3. Построить вещественные признаки степени 2.\n",
    "4. Обучить модель на совокупности признаков из п2 и отфильтрованных категориальных. Сравнить ее с предыдущими моделями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# место для вашего кода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# место для вашего кода"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Hide code",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
